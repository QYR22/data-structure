以下为题目-任务版，原题见pdf文件。
# Task1：
## 题目要求
- 三种数据规模：200条、4000条、8\*10^5条。
- 每条输入数据格式：商家名称*字符串* 位置_x*int类型* 位置_y*int类型* 菜系*字符串*
- 输入：
```
输入条目数n 查询条目数m
n行输入数据
m行查询数据(即商家名称)
```
- 输出：m行，每行为对应商家信息，每行↓
```
位置_x 位置_y // 菜系
```
## task1 编写过程总结
200、4000 组合都不是大的问题，主要在于八十万这一组数据，数据量庞大，还有升序、倒序、乱序，各种组合。只用一个 AVL 树的时候，树会很深，需要的处理也多，所以
是有很大优化空间的。单纯只用哈希表，冲突处理也很有讲究，要有序排列下来便于查询；除了冲突处理，查询的时候也要尽可能节省时间，于是我选择了二分查找，相对于顺序查找
的 O(n)，二分查找能够控制在 log2 的复杂度范围内即使在八十万这一组数据的考验下，也不会耗费太多时间。

---
# Task2：
## 题目要求
- 第一行以空格分隔的整数n，代表顶点个数, 0 <= n <= 4000
- 第二行到第n+1行是顶点的坐标x和y，均为浮点数, 0<= x,y <= 10000
- 输出格式：使得混合-k近邻图连通的最小k值
- 数据规模：100、500、1500 等等；数据在空间中随机分布。为了客观起见，一个数据规模下可以产生多组数据，然后计算其平均值。
## task1 编写过程总结
一开始没有建图，每一轮更新 k 之后都要重复之前的 k 已经做过的判断这样累加下来，重复而无效的次数数不胜数，所以需要通过中间的建图这一步骤来节约判断耗费掉的时间。
同时如果使用 heapsort，对第 k 近的点就可以之间通过下标索引进行访问了，而不需要像 AVL 树这样还需要向下查找。


# 总结
总之，本次大作业就是经过重重超时一步一步向上爬，对查找算法更加熟悉，对哈希/AVL/heap 这些存储结构更加了解，在思想上也收获了很多。
心得体会：
  首先，在做任务一的过程中，始终在和超时奋战…过程很艰辛，但是我也收获了很多。遇到问题的时候确实应该先分析分析，我有什么地方是好的，我应该保留下来；我觉得我有
什么地方好像不太好——那是为什么不太好，进入深度的分析（还有什么地方可以改进，为什么可以这么改进）。
在第一个任务中得到的这一体会，让我在第二任务中，相对于第一个任务节约了比较多的紧盯屏幕的时间和精力，相反，是更多的在大脑中的思考，我觉得这对于平时的写代码也
是很重要的，如果毫无头绪就开始写效率是非常低下的，需要在头脑中有一个大概的逻辑再开始。
还有一点，不能把自己的思维固化在一个方面了，就比如说任务 1 中 AVL 有着良好的表现，我就在任务二中首先采用了 AVL，通过 task2 的时间测试可以很明显的看出这是没有
必要的，因为观察 AVL 的结构、将其与 heap 结构对比可以看出任务二中用它并不是很合适。
同时，经过本次大作业的训练，我对排序、查找的认识更加深刻，体会到在庞大的数据量下，不同的算法的差距可以如此大，而一个好的算法，可以为我们的生产生活、甚至是科
学上的进步做出如此大的突破。
